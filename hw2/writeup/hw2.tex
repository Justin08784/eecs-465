\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}
\usepackage[a4paper,margin=3.5cm]{geometry} %sets the page geometry
\usepackage{url}
\usepackage{dirtytalk}
\usepackage{graphicx} % package for \includegraphics
\usepackage{wrapfig} % figure wrapping
\setlength{\parskip}{1em} % set space when paragraphs are used
\setlength{\parindent}{0pt}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{mathtools}

% lets you use \blankpage to make a blank page
\newcommand{\blankpage}{
\newpage
\thispagestyle{empty}
\mbox{}
\newpage
}

% self explanatory
\theoremstyle{definition}
\newtheorem{theorem}{theorem}[section]
\newtheorem{definition}{definition}[section]
\newtheorem{corollary}{corollary}[theorem]
\newtheorem{lemma}[theorem]{lemma}


% other
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor} %floor function

\begin{document}
\textbf{1. (5 points) Is a single point convex? Use the definition of convexity to justify your answer.}

Yes, a singleton is a convex set.

\begin{proof} 
Recall that a set $S \subseteq \mathbb{R}^n$ is convex \textit{iff}
$x, y \in S, \theta \in [0, 1] \implies \theta x + (1 - \theta) y \in S$

Consider the singleton $\{a\}\subseteq\mathbb{R}^n$. Fix $x, y \in \{a\}, \theta \in [0,1]$.

ETS $\theta x + (1 - \theta) y \in \{a\}$

Since $a$ is the only element of $\{a\}$, $x = y = a$.

Thus, $\theta x + (1 - \theta) y = \theta a + (1 - \theta) a = a \in \{a\}$
\end{proof} 

\bigskip
\textbf{2. (10 points) Is the function...}

Yes, the function is convex.

\begin{proof}

$f(x) = \lvert 2 - 5x\rvert + 2x + 8e^{-4x} - 1$

Define:

$f_1(x) \coloneq 2 - 5x$ (affine)

$f_2(x) \coloneq \lvert x \rvert$ (absolute value)

$f_3(x) \coloneq 2x$ (affine)

$f_4(x) \coloneq e^{-4x}$ (exponential)

$f_5(x) \coloneq - 1$ (affine)

These are all convex functions according to slide 31 of the "Convexity and Optimization" lecture on 09/09.

$f = f_2 \circ f_1 + f_3 + 8f_4 + f_5$

(1) $f_1 \text{ is affine } \land f_2 \text{ is convex} \implies f_2 \circ f_1 \text{ is convex}$
% (composing affine map with convex function is convex)

(2) $8 \geq 0 \land f_4 \text{ is convex} \implies 8f_4 \text{ is convex}$
% (non-negative scalar multiplication preserves convexity)

Thus, $f$ is a sum of convex functions.
Since summation preserves convexity, $f$ is convex.
\end{proof}


\bigskip
\textbf{3. (10 points) Rewrite the following optimization problem...}

\[
\begin{aligned}
    \text{minimize}_{x} \quad & -3x_1 + 4x_2 + 3 \\
    \text{subject to} \quad & x_2 + 3 \leq 0, \\
                            & 2x_1 - 5x_2 + 2 \leq 0, \\
                            & -x_2 + 5 \leq 0, \\
                            & x_1 - x_2 - 6.3 = 0.
\end{aligned}
\]

\bigskip
\textbf{4. (15 points) Consider...}

$\delta f$ contains more than 1 value only at $\{-\frac{1}{3}, 1\}$

$\delta f(-\frac{1}{3}) = [-2, 2]$

$\delta f(1) = [2, 6]$

\begin{proof}

$f = max\{g, h\} \text{ where } g(x) \coloneq 3x^2 - 2, h(x) \coloneq 2x - 1$

$g'(x) = 6x$

$h'(x) = 2$

$g,h$ are convex and pointwise maximum preserves convexity

$\implies f$ is convex

$\implies \delta f \neq \varnothing$ everywhere.

Line $h$ makes two intersections with $g$:

$g = h \iff 3x^2 - 2 = 2x - 1 \iff (3x + 1)(x - 1) = 0 \iff x \in \{-\frac{1}{3}, 1\}$

Due to strict convexity of $g$, \textbf{(A)} $g < h$ on $(-\frac{1}{3}, 1)$ and $h < g$ everywhere else.

$g, h$ differentiable everywhere $\land \textbf{ (A)}$

$\implies f$ is differentiable on $\mathbb{R} \setminus \{-\frac{1}{3}, 1\}$

$\implies \forall x \in \mathbb{R} \setminus \{-\frac{1}{3}, 1\}, \delta f(x) = \{f'(x)\} \text{ (a singleton)}$

Thus, $\delta f$ can contain multiple elements only on $\{-\frac{1}{3}, 1\}$.

$\textbf{(1) } g'(-\frac{1}{3}) = -2 \neq 2 = h'(-\frac{1}{3})$

$\textbf{(2) } g'(1) = 6 \neq 2 = h'(1)$

\textbf{(1), (2)} imply that $f$ has non-differentiable kinks at $-\frac{1}{3}$ and $1$.

Thus, on $\{-\frac{1}{3}, 1\}, \delta f = [\min\{g', h'\},\max\{g', h'\}]$.

$\delta f(-\frac{1}{3}) = [-2, 2]$

$\delta f(1) = [2, 6]$

\end{proof}


\bigskip
\textbf{5. A linear program...}

We assume that:
$x \in \mathbb{R}^n, h \in \mathbb{R}^m, b \in \mathbb{R}^p$

Thus, $c \in \mathbb{R}^{n \times 1}$, $G = (G_{ij}) \in \mathbb{R}^{m \times n}$, $A = (A_{ij}) \in \mathbb{R}^{p \times n}$

\textbf{a. (10 points) Write down...}

\[
    g:\mathbb{R}^m \times \mathbb{R}^p, \\
    g(\lambda, \nu) = \inf_{x\in\mathbb{R}^n}{
        c^Tx + \lambda^{T}(Gx - h) + \nu^{T}(Ax - b)
    }
\]

\textbf{b. (5 points) Write down...}

\[
\begin{aligned}
    \text{maximize}_{\lambda, \nu} \quad & g(\lambda, \nu) \\
    \text{subject to} \quad & \lambda \succeq 0
\end{aligned}
\]


\textbf{c. (5 points) Suppose you solved...}

$d^* = p^*$

\begin{proof}

Firstly, we can rewrite the primal problem as:
\[
\begin{aligned}
    \text{minimize}_{x} \quad & c^Tx \\
    \text{subject to} \quad & f_i \leq 0 \quad & 1\leq i\leq m\\
                      \quad & Ax = b
\end{aligned}
\]
\text{...where } $\forall 1 \leq i \leq m, \quad f_i(x) \coloneq G^T_{i}x - h_i$, 
and $G_i$ is the ith \textit{row} of G.

% \textbf{Primal feasibility}:
% 
% The feasible set of the primal problem is: 
% \[
% \mathcal{D}_f = \{x\in\mathbb{R}^n \mid f_i(x) \leq 0, \forall 1\leq i\leq m \land Ax = b\}
% \]
% 
% 
% Each $f_i \leq 0$ prescribes a half-space (for $m$ in total), while $Ax = b$ prescribes a p-size collection of hyperplanes.
% 
% Since, $\mathcal{D}_f$ is a finite intersection of half-spaces and hyperplanes, each of which is closed,
% $\mathcal{D}_f$ is closed.
% 
% Further, it was given that the primal problem is bounded or, equivalently, that $\mathcal{D}_f$ is bounded.
% 
% Thus $\mathcal{D}_f$ is compact.
% 
% 
% By EVT, since $x \mapsto c^Tx$ is affine (and therefore continuous),
% it attains a minimum value ($p^*$) on compact $\mathcal{D}_f$.

% \textbf{Strong duality}:



Slater's theorem states that strong duality holds (i.e. $p^* = d^*$) \textit{if} the problem is convex
and Slater's condition holds.

Since the objective function and constraint functions are all affine maps, and thus convex,
the primal problem is convex.

Slater's condition for affine inequality constraints (Boyd 5.27) is satisfied \textit{iff}
\[
    (\star) \exists x\in \textbf{relint}(\mathcal{D}) \text{ s.t. } \forall 1\leq i\leq m 
    \quad f_i \leq 0, \quad Ax = b
\]


$\textbf{relint}(\mathcal{D}) = \textbf{relint}(\mathbb{R}^n) = \mathbb{R}^n = \mathcal{D}$


Thus, Slater's condition reduces to:
\[
    (\star) \exists x\in \mathcal{D} \text{ s.t. } \forall 1\leq i\leq m 
    \quad f_i \leq 0, \quad Ax = b
\]

This is equivalent to primal feasibility, which was given.

By Slater's theorem, strong duality holds.
\end{proof}



\textbf{Software 1. }

\textbf{d. (20 points) Run Gradient Descent and Newton's method...}

\textbf{Q. Explain which algorithm performed better in this example in terms of number of iterations and why.}

Newton's method performed better because it converged to the
optimum (within epsilon) faster: in only 5 iterations vs.
17 iterations for gradient descent.

Newton's method computes $\Delta x$ using both 1st and 2nd derivatives 
(instead of just 1st derivative as in gradient descent).
Richer curvature information encoded in the 2nd derivative allows Newton's 
method to take more aggressive and well-tuned steps towards local optimum.

\textbf{Software 3. }

\textbf{a. (5 points) Write down...}

Suppose that the primal problem is:
\[
\begin{aligned}
    \text{minimize}_{x} \quad & f_0(x) \\
    \text{subject to} \quad & f_i(x) \leq 0 \quad & 1 \leq i \leq m\\
                            & h_i(x) = 0    \quad & 1 \leq i \leq p 
\end{aligned}
\]

Then the barrier method reformulation is: 
\[
\begin{aligned}
    \text{minimize}_{x} \quad & tf_0(x) + \phi(x)\\
    \text{subject to} \quad & h_i(x) = 0 \quad & 1 \leq i \leq p\\
\end{aligned}
\]

\[
\begin{aligned}
    \phi & : \mathcal{D} \rightarrow \mathbb{R}, \text{ where }\\
    \mathcal{D} & \coloneq \{x \in \mathbb{R} ^ n \mid \forall 1\leq i\leq m, f_i(x) < 0\}\\
    \forall & x \in \mathcal{D}, \phi(x) \coloneq \sum_{i=1}^{m} -log(-f_i(x))
\end{aligned}
\]

$x \mapsto tf_0(x) + \phi(x)$ is the new objective function,
with optimization variable $x$ and objective-barrier weight $t$.

\textbf{b. (5 points) Write down...}


Suppose that the primal problem is:
\[
\begin{aligned}
    & \text{minimize}_{x} \quad & c^Tx \\
    & \text{subject to} \quad & Gx \preceq h \\
    & \quad & Ax = b  
\end{aligned}
\]

...where:

$x \in \mathbb{R}^n, h \in \mathbb{R}^m, b \in \mathbb{R}^p$

$c \in \mathbb{R}^{n \times 1}$, $G = (G_{ij}) \in \mathbb{R}^{m \times n}$, $A = (A_{ij}) \in \mathbb{R}^{p \times n}$

Then the barrier method reformulation is: 
\[
\begin{aligned}
    \text{minimize}_{x} \quad & tc^Tx + \phi(x)\\
    \text{subject to} \quad & Ax = b \quad & 1 \leq i \leq p\\
\end{aligned}
\]

\[
\begin{aligned}
    \phi & : \mathcal{D} \rightarrow \mathbb{R}, \text{ where }\\
    \mathcal{D} & \coloneq \{x \in \mathbb{R} ^ n \mid \forall 1\leq i\leq m, G_i^Tx - h_i < 0\}\\
    \forall & x \in \mathcal{D}, \phi(x) \coloneq \sum_{i=1}^{m} -log(-(G_i^Tx - h_i))
\end{aligned}
\]

$x \mapsto tc^Tx + \phi(x)$ is the new objective function,
with optimization variable $x$ and objective-barrier weight $t$.

\end{document}

