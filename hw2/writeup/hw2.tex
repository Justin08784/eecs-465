\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}
\usepackage[a4paper,margin=3.5cm]{geometry} %sets the page geometry
\usepackage{url}
\usepackage{dirtytalk}
\usepackage{graphicx} % package for \includegraphics
\usepackage{wrapfig} % figure wrapping
\setlength{\parskip}{1em} % set space when paragraphs are used
\setlength{\parindent}{0pt}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{mathtools}

% lets you use \blankpage to make a blank page
\newcommand{\blankpage}{
\newpage
\thispagestyle{empty}
\mbox{}
\newpage
}

% self explanatory
\theoremstyle{definition}
\newtheorem{theorem}{theorem}[section]
\newtheorem{definition}{definition}[section]
\newtheorem{corollary}{corollary}[theorem]
\newtheorem{lemma}[theorem]{lemma}


% other
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor} %floor function

\begin{document}
\subsection*{1. (5 points) Is a single point convex? Use the definition of convexity to justify your answer.}

Yes, a singleton is a convex set.

\begin{proof} 
Recall that a set $S \subseteq \mathbb{R}^n$ is convex \textit{iff}
$x, y \in S, \theta \in [0, 1] \implies \theta x + (1 - \theta) y \in S$

Consider the singleton $\{a\}\subseteq\mathbb{R}^n$. Fix $x, y \in \{a\}, \theta \in [0,1]$.

ETS $\theta x + (1 - \theta) y \in \{a\}$

Since $a$ is the only element of $\{a\}$, $x = y = a$.

Thus, $\theta x + (1 - \theta) y = \theta a + (1 - \theta) a = a \in \{a\}$
\end{proof} 

\bigskip
\subsection*{2. (10 points) Is the function...}

Yes, the function is convex.

\begin{proof}

$f(x) = \lvert 2 - 5x\rvert + 2x + 8e^{-4x} - 1$

Define:

$f_1(x) \coloneq 2 - 5x$ (affine)

$f_2(x) \coloneq \lvert x \rvert$ (absolute value)

$f_3(x) \coloneq 2x$ (affine)

$f_4(x) \coloneq e^{-4x}$ (exponential)

$f_5(x) \coloneq - 1$ (affine)

These are all convex functions according to slide 31 of the "Convexity and Optimization" lecture on 09/09.

$f = f_2 \circ f_1 + f_3 + 8f_4 + f_5$

(1) $f_1 \text{ is affine } \land f_2 \text{ is convex} \implies f_2 \circ f_1 \text{ is convex}$
% (composing affine map with convex function is convex)

(2) $8 \geq 0 \land f_4 \text{ is convex} \implies 8f_4 \text{ is convex}$
% (non-negative scalar multiplication preserves convexity)

Thus, $f$ is a sum of convex functions.
Since summation preserves convexity, $f$ is convex.
\end{proof}


\bigskip
\subsection*{3. (10 points) Rewrite the following optimization problem...}

\[
\begin{aligned}
    \text{minimize}_{x} \quad & -3x_1 + 4x_2 + 3 \\
    \text{subject to} \quad & x_2 + 3 \leq 0, \\
                            & 2x_1 - 5x_2 + 2 \leq 0, \\
                            & -x_2 + 5 \leq 0, \\
                            & x_1 - x_2 - 6.3 = 0.
\end{aligned}
\]

\bigskip
\subsection*{4. (15 points) Consider...}

$\delta f$ contains more than 1 value only at $\{-\frac{1}{3}, 1\}$

$\delta f(-\frac{1}{3}) = [-2, 2]$

$\delta f(1) = [2, 6]$

\begin{proof}

$f = max\{g, h\} \text{ where } g(x) \coloneq 3x^2 - 2, h(x) \coloneq 2x - 1$

$g'(x) = 6x$

$h'(x) = 2$

$g,h$ are convex and pointwise maximum preserves convexity

$\implies f$ is convex

$\implies \delta f \neq \varnothing$ everywhere.

Line $h$ makes two intersections with $g$:

$g = h \iff 3x^2 - 2 = 2x - 1 \iff (3x + 1)(x - 1) = 0 \iff x \in \{-\frac{1}{3}, 1\}$

Due to strict convexity of $g$, \textbf{(A)} $g < h$ on $(-\frac{1}{3}, 1)$ and $h < g$ everywhere else.

$g, h$ differentiable everywhere $\land \textbf{ (A)}$

$\implies f$ is differentiable on $\mathbb{R} \setminus \{-\frac{1}{3}, 1\}$

$\implies \forall x \in \mathbb{R} \setminus \{-\frac{1}{3}, 1\}, \delta f(x) = \{f'(x)\} \text{ (a singleton)}$

Thus, $\delta f$ can contain multiple elements only on $\{-\frac{1}{3}, 1\}$.

$\textbf{(1) } g'(-\frac{1}{3}) = -2 \neq 2 = h'(-\frac{1}{3})$

$\textbf{(2) } g'(1) = 6 \neq 2 = h'(1)$

\textbf{(1), (2)} imply that $f$ has non-differentiable kinks at $-\frac{1}{3}$ and $1$.

Thus, on $\{-\frac{1}{3}, 1\}, \delta f = [\min\{g', h'\},\max\{g', h'\}]$.

$\delta f(-\frac{1}{3}) = [-2, 2]$

$\delta f(1) = [2, 6]$

\end{proof}


\bigskip
\subsection*{5. A linear program...}

We assume that:
$x \in \mathbb{R}^n, h \in \mathbb{R}^m, b \in \mathbb{R}^p$

Thus, $c \in \mathbb{R}^{n \times 1}$, $G = (G_{ij}) \in \mathbb{R}^{m \times n}$, $A = (A_{ij}) \in \mathbb{R}^{p \times n}$

\textbf{a. (10 points) Write down...}

\[
    g:\mathbb{R}^m \times \mathbb{R}^p, \\
    g(\lambda, \nu) = \inf_{x\in\mathbb{R}^n}{
        c^Tx + \lambda^{T}(Gx - h) + \nu^{T}(Ax - b)
    }
\]

\textbf{b. (5 points) Write down...}

\[
\begin{aligned}
    \text{maximize}_{\lambda, \nu} \quad & g(\lambda, \nu) \\
    \text{subject to} \quad & \lambda \succeq 0
\end{aligned}
\]


\textbf{c. (5 points) Suppose you solved...}

$d^* = p^*$

\begin{proof}

Firstly, we can rewrite the primal problem as:
\[
\begin{aligned}
    \text{minimize}_{x} \quad & c^Tx \\
    \text{subject to} \quad & f_i \leq 0 \quad & 1\leq i\leq m\\
                      \quad & Ax = b
\end{aligned}
\]
\text{...where } $\forall 1 \leq i \leq m, \quad f_i(x) \coloneq G^T_{i}x - h_i$, 
and $G_i$ is the ith \textit{row} of G.

% \textbf{Primal feasibility}:
% 
% The feasible set of the primal problem is: 
% \[
% \mathcal{D}_f = \{x\in\mathbb{R}^n \mid f_i(x) \leq 0, \forall 1\leq i\leq m \land Ax = b\}
% \]
% 
% 
% Each $f_i \leq 0$ prescribes a half-space (for $m$ in total), while $Ax = b$ prescribes a p-size collection of hyperplanes.
% 
% Since, $\mathcal{D}_f$ is a finite intersection of half-spaces and hyperplanes, each of which is closed,
% $\mathcal{D}_f$ is closed.
% 
% Further, it was given that the primal problem is bounded or, equivalently, that $\mathcal{D}_f$ is bounded.
% 
% Thus $\mathcal{D}_f$ is compact.
% 
% 
% By EVT, since $x \mapsto c^Tx$ is affine (and therefore continuous),
% it attains a minimum value ($p^*$) on compact $\mathcal{D}_f$.

% \textbf{Strong duality}:



Slater's theorem states that strong duality holds (i.e. $p^* = d^*$) \textit{if} the problem is convex
and Slater's condition holds.

Since the objective function and constraint functions are all affine maps, and thus convex,
the primal problem is convex.

Slater's condition for affine inequality constraints (Boyd 5.27) is satisfied \textit{iff}
\[
    (\star) \exists x\in \textbf{relint}(\mathcal{D}) \text{ s.t. } \forall 1\leq i\leq m 
    \quad f_i \leq 0, \quad Ax = b
\]


$\textbf{relint}(\mathcal{D}) = \textbf{relint}(\mathbb{R}^n) = \mathbb{R}^n = \mathcal{D}$


Thus, Slater's condition reduces to:
\[
    (\star) \exists x\in \mathcal{D} \text{ s.t. } \forall 1\leq i\leq m 
    \quad f_i \leq 0, \quad Ax = b
\]

This is equivalent to primal feasibility, which was given.

By Slater's theorem, strong duality holds.
\end{proof}



\subsection*{Software 1. }

\textbf{d. (20 points) Run Gradient Descent and Newton's method...}

\textbf{Q. Explain which algorithm performed better in this example in terms of number of iterations and why.}

Newton's method performed better because it converged to the
optimum (within epsilon) faster: in only 5 iterations vs.
17 iterations for gradient descent.

Newton's method computes $\Delta x$ using both 1st and 2nd derivatives 
(instead of just 1st derivative as in gradient descent).
Richer curvature information encoded in the 2nd derivative allows Newton's 
method to take more aggressive and well-tuned steps towards local optimum.

\subsection*{Software 2.}

\textbf{b. (10 points) Create a plot...}

\textbf{Is fsum($x_i$) strictly decreasing? Explain why or why not.}

fsum($x_i$) is NOT strictly decreasing

Unlike gradient descent, which computes the exact gradient over fsum
–– and thus over all of the cost functions (i.e. the fi's)–– to determine its next step, 
SGD approximates the gradient by computing it over a random subset
of the cost functions (in our case just 1 fi at a time).

The gradient provides the direction of steepest descent. Since SGD steps using 
an approximated gradient (based on one fi), each step is noisy and may not 
necessarily follow the direction of steepest descent for fsum. As a result, any 
individual update can cause an increase (or non-decrease) in fsum, even though 
the overall trend tends to minimize fsum in the long run.

\textbf{c. (10 points) Run SGD 30 times...}
\begin{verbatim}
t = 1:
750 iterations:
    var = 6.271
    mean = -1403.251
1000 iterations:
    var = 7.560
    mean = -1403.201

t = 0.5:
750 iterations:
    var = 55.857
    mean = -1350.145
1000 iterations:
    var = 6.743
    mean = -1402.073
\end{verbatim}

\textbf{Analysis t = 1:}

The 30-run mean is virtually identical for both iteration counts. The 30-run variances fluctuate: sometimes higher for 750 iterations, sometimes lower.

This suggests that by 750 iterations, SGD has likely converged to a local optimum.
Thus, additional iterations don't reduce the objective function further but instead cause the
algorithm to "wander" around the optimum.

\textbf{Analysis t = 0.5:}

If we lower t to t = 0.5, then both 30-run mean and variance are lower for
1k iterations. With the reduced step size, 750 iterations may not be sufficient for convergence,
explaining the less-optimal mean for 750 iterations. Additionally, if SGD is
still making relatively large steps at iteration 750, small differences in the stochastic
updates could cause noticeable differences in the final objective value, explaining
the larger variance for 750 iterations.

\textbf{d. Now we will compare SGD...}
\begin{verbatim}
Gradient Descent (1 run):
    fsum_x = -1405.267
    num_iter = 19
    runtime = 7.224

Newton's Method (1 run):
    fsum_x = -1405.267
    num_iter = 4
    runtime = 0.344

SGD (30 runs):
    fsum_x var = 6.121
    fsum_x mean = -1403.486
    num_iter = 1000
    av. runtime = 0.015
\end{verbatim}

Note:
\begin{itemize}
    \item SGD was run 30 times; average and variance were taken
    \item GD and NT were run only once each since they are deterministic.
\end{itemize}


\textbf{i) (10 points)}
runtime: SGD $<$ NT $<$ GD

Although SGD has many more iterations than NT and GD, each iteration is
orders of magnitudes cheaper because it only requires differentiating a single fi function,
rather than the entire fsum function as in NT and GD.

NT is faster than GD because it considers the 2nd derivative (in addition to the 1st) to
determine the next step. This has two effects:
\begin{enumerate}
    \item Faster convergence: Each update is more aggressive and well-tuned, 
        bringing the $x_i$ sequence closer to local optimum with minimal overshooting. 
        This means fewer iterations needed for convergence.
    \item Cheaper iterations: Since the update direction dx incorporates 2nd derivative curvature
information, the steps are better tuned. Thus, per iteration, fewer iterations of backtracking
line search is required to choose an appropriate step-size t.
\end{enumerate}

\textbf{ii) (10 points)}
GD and NT have near-identical fsum values.
This is an inevitable consequence of the epsilon-based stopping condition,
which guarantees that the final fsum value is extremely close to local optimum.

Furthermore, SGD, despite it being an approximate, stochastic method, (unlike
the exact NT and GD algorithms) has an (average) fsum value that is very
close to that of GD and NT. This shows that SGD was able to converge sufficiently
to local optimum within 1000 iterations.

The difference in fsum values are not significant.

\subsection*{Software 3. }

\textbf{a. (5 points) Write down...}

Suppose that the primal problem is:
\[
\begin{aligned}
    \text{minimize}_{x} \quad & f_0(x) \\
    \text{subject to} \quad & f_i(x) \leq 0 \quad & 1 \leq i \leq m\\
                            & h_i(x) = 0    \quad & 1 \leq i \leq p 
\end{aligned}
\]

Then the barrier method reformulation is: 
\[
\begin{aligned}
    \text{minimize}_{x} \quad & tf_0(x) + \phi(x)\\
    \text{subject to} \quad & h_i(x) = 0 \quad & 1 \leq i \leq p\\
\end{aligned}
\]

\[
\begin{aligned}
    \phi & : \mathcal{D} \rightarrow \mathbb{R}, \text{ where }\\
    \mathcal{D} & \coloneq \{x \in \mathbb{R} ^ n \mid \forall 1\leq i\leq m, f_i(x) < 0\}\\
    \forall & x \in \mathcal{D}, \phi(x) \coloneq \sum_{i=1}^{m} -log(-f_i(x))
\end{aligned}
\]

$x \mapsto tf_0(x) + \phi(x)$ is the new objective function,
with optimization variable $x$ and objective-barrier weight $t$.

\textbf{b. (5 points) Write down...}


Suppose that the primal problem is:
\[
\begin{aligned}
    & \text{minimize}_{x} \quad & c^Tx \\
    & \text{subject to} \quad & Gx \preceq h \\
    & \quad & Ax = b  
\end{aligned}
\]

...where:

$x \in \mathbb{R}^n, h \in \mathbb{R}^m, b \in \mathbb{R}^p$

$c \in \mathbb{R}^{n \times 1}$, $G = (G_{ij}) \in \mathbb{R}^{m \times n}$, $A = (A_{ij}) \in \mathbb{R}^{p \times n}$

Then the barrier method reformulation is: 
\[
\begin{aligned}
    \text{minimize}_{x} \quad & tc^Tx + \phi(x)\\
    \text{subject to} \quad & Ax = b \quad & 1 \leq i \leq p\\
\end{aligned}
\]

\[
\begin{aligned}
    \phi & : \mathcal{D} \rightarrow \mathbb{R}, \text{ where }\\
    \mathcal{D} & \coloneq \{x \in \mathbb{R} ^ n \mid \forall 1\leq i\leq m, G_i^Tx - h_i < 0\}\\
    \forall & x \in \mathcal{D}, \phi(x) \coloneq \sum_{i=1}^{m} -log(-(G_i^Tx - h_i))
\end{aligned}
\]

$x \mapsto tc^Tx + \phi(x)$ is the new objective function,
with optimization variable $x$ and objective-barrier weight $t$.

\end{document}

